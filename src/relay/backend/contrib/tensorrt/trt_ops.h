/* * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef TVM_RELAY_BACKEND_TRT_OPS_H_
#define TVM_RELAY_BACKEND_TRT_OPS_H_

#include <stdlib.h>
#include <tvm/relay/attrs/nn.h>
#include <tvm/relay/expr_functor.h>
#include <tvm/relay/transform.h>
#include <tvm/relay/type.h>
#include <tvm/runtime/module.h>
#include <tvm/runtime/ndarray.h>

#include <unordered_map>
#include <vector>
#include "NvInfer.h"

namespace tvm {
namespace relay {
namespace contrib {

enum TrtInputType {
  kTensor,
  kWeight,
};

struct TrtOpInput {
  TrtInputType type;
  nvinfer1::ITensor* tensor;
  nvinfer1::Weights weight;
  std::vector<int> weight_shape;

  TrtOpInput(nvinfer1::ITensor* tensor) : tensor(tensor), type(kTensor) {}
  TrtOpInput(nvinfer1::Weights weight, const std::vector<int>& shape) : weight(weight), type(kWeight), weight_shape(shape) {}
};

// Parameters to convert an Op from relay to TensorRT
struct AddTrtLayerParams {
  const CallNode* call;
  nvinfer1::INetworkDefinition* network;
  std::string op_name;
  std::vector<TrtOpInput> inputs;
  std::vector<nvinfer1::ITensor*> outputs;

  AddTrtLayerParams(nvinfer1::INetworkDefinition* network, const CallNode* call) : network(network), call(call) {
    op_name = (call->op.as<OpNode>())->name;
  }
};


class TrtOpConverter {
public:
  const std::vector<TrtInputType> input_types;

  TrtOpConverter(const std::vector<TrtInputType>& input_types) : input_types(input_types) {}

  virtual void Convert(AddTrtLayerParams& params) const = 0;

  // Helper functions
  nvinfer1::ITensor* Reshape(AddTrtLayerParams& params, nvinfer1::ITensor* input, const std::vector<int>& new_shape) const {
    auto layer = params.network->addShuffle(*input);
    CHECK(layer != nullptr);
    layer->setReshapeDimensions(VectorToTrtDims(new_shape));
    return layer->getOutput(0);
  }
};

class ActivationOpConverter : public TrtOpConverter {
public:
  ActivationOpConverter() : TrtOpConverter({kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    CHECK(params.inputs.size() == 1) << "Activation op expects 1 input.";
    static const std::unordered_map<std::string, nvinfer1::ActivationType>
        op_map = {{"nn.relu", nvinfer1::ActivationType::kRELU},
                  {"sigmoid", nvinfer1::ActivationType::kSIGMOID},
                  {"tanh", nvinfer1::ActivationType::kTANH},
                  {"clip", nvinfer1::ActivationType::kCLIP},
                  {"nn.leaky_relu", nvinfer1::ActivationType::kLEAKY_RELU},
                  };
    auto it = op_map.find(params.op_name);
    CHECK(it != op_map.end()) << "Unsupported activation type " << params.op_name;
    nvinfer1::IActivationLayer* act_layer =
        params.network->addActivation(*params.inputs.at(0).tensor, nvinfer1::ActivationType::kRELU);
    if (params.op_name == "clip") {
      // Set clip parameterss.
      const auto* clip_attr = params.call->attrs.as<ClipAttrs>();
      act_layer->setAlpha(clip_attr->a_min);
      act_layer->setAlpha(clip_attr->a_max);
    } else if (params.op_name == "nn.leaky_relu") {
      const auto* leaky_relu_attr = params.call->attrs.as<LeakyReluAttrs>();
      act_layer->setAlpha(leaky_relu_attr->alpha);
    }
    CHECK(act_layer != nullptr);
    params.outputs.push_back(act_layer->getOutput(0));
  }
};

class ElementWiseBinaryOpConverter : public TrtOpConverter {
public:
  ElementWiseBinaryOpConverter() : TrtOpConverter({kTensor, kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    static const std::unordered_map<std::string, nvinfer1::ElementWiseOperation> op_map =
        {{"add", nvinfer1::ElementWiseOperation::kSUM},
        {"subtract", nvinfer1::ElementWiseOperation::kSUB},
        {"multiply", nvinfer1::ElementWiseOperation::kPROD},
        {"divide", nvinfer1::ElementWiseOperation::kDIV},
        {"power", nvinfer1::ElementWiseOperation::kPOW},
        {"nn.bias_add", nvinfer1::ElementWiseOperation::kSUM}};
    auto it = op_map.find(params.op_name);
    CHECK(it != op_map.end()) << "Unsupported elementwise type " << params.op_name;

    // Broadcast
    auto input0 = params.inputs.at(0).tensor;
    auto input0_dims = TrtDimsToVector(input0->getDimensions());
    auto input1 = params.inputs.at(1).tensor;
    auto input1_dims = TrtDimsToVector(input1->getDimensions());
    const bool need_broadcast = input0_dims.size() != input1_dims.size();
    if (need_broadcast) {
      if (input0_dims.size() < input1_dims.size()) {
        std::vector<int> new_shape(input0_dims);
        while (new_shape.size() < input1_dims.size()) new_shape.insert(new_shape.begin(), 1);
        input0 = Reshape(params, input0, new_shape);
      } else if (input1_dims.size() < input0_dims.size()) {
        std::vector<int> new_shape(input1_dims);
        while (new_shape.size() < input0_dims.size()) new_shape.insert(new_shape.begin(), 1);
        input1 = Reshape(params, input1, new_shape);
      } 
    }

    nvinfer1::IElementWiseLayer* elemwise_layer =
        params.network->addElementWise(*input0, *input1, it->second);
    CHECK(elemwise_layer != nullptr);
    params.outputs.push_back(elemwise_layer->getOutput(0));
  }
};

class Conv2DOpConverter : public TrtOpConverter {
public:
  Conv2DOpConverter() : TrtOpConverter({kTensor, kWeight}) { }

  void Convert(AddTrtLayerParams& params) const {
    auto input_tensor = params.inputs.at(0).tensor;
    auto input_dims = TrtDimsToVector(input_tensor->getDimensions());
    auto weight_shape = params.inputs.at(1).weight_shape;
    const auto* conv2d_attr = params.call->attrs.as<Conv2DAttrs>();
    CHECK(conv2d_attr->data_layout == "NCHW");
    if (auto channels = conv2d_attr->channels.as<IntImm>()) {
      CHECK_EQ(channels->value, weight_shape[0]) << "debug: need to use channels";
    }

    const int num_outputs = weight_shape[0]; //* conv2d_attr->groups;
    const auto kernel_size = nvinfer1::DimsHW(weight_shape[2], weight_shape[3]);
    nvinfer1::Weights bias{nvinfer1::DataType::kFLOAT, nullptr, 0};
    auto conv_layer = params.network->addConvolution(*params.inputs.at(0).tensor, num_outputs, kernel_size, params.inputs.at(1).weight, bias);
    CHECK(conv_layer != nullptr);

    const auto padding = nvinfer1::DimsHW(conv2d_attr->padding[0].as<IntImm>()->value, conv2d_attr->padding[1].as<IntImm>()->value);
    conv_layer->setPadding(padding);
    const auto strides = nvinfer1::DimsHW(conv2d_attr->strides[0].as<IntImm>()->value, conv2d_attr->strides[1].as<IntImm>()->value);
    conv_layer->setStride(strides);
    conv_layer->setNbGroups(conv2d_attr->groups);
    params.outputs.push_back(conv_layer->getOutput(0));
  }
};

// Using FullyConnected
class DenseOpConverter : public TrtOpConverter {
public:
  DenseOpConverter() : TrtOpConverter({kTensor, kWeight}) { }

  void Convert(AddTrtLayerParams& params) const {
    auto input_tensor = params.inputs.at(0).tensor;
    auto input_dims = TrtDimsToVector(input_tensor->getDimensions());
    CHECK(input_dims.size() > 0 && input_dims.size() <= 3);
    const bool need_reshape_on_input = input_dims.size() != 3;
    if (need_reshape_on_input) {
      // Add dims of size 1 until rank is 3.
      std::vector<int> new_shape(input_dims);
      while (new_shape.size() < 3) new_shape.insert(new_shape.begin(), 1);
      input_tensor = Reshape(params, input_tensor, new_shape);
    }
    // Weights are in KC format.
    CHECK(params.inputs.at(1).weight_shape.size() == 2);
    const int num_units = params.inputs.at(1).weight_shape[0];
    nvinfer1::Weights bias{nvinfer1::DataType::kFLOAT, nullptr, 0};
    nvinfer1::IFullyConnectedLayer* fc_layer = params.network->addFullyConnected(*input_tensor, num_units, params.inputs.at(1).weight, bias);
    CHECK(fc_layer != nullptr);
    auto output_tensor = fc_layer->getOutput(0);
    if (need_reshape_on_input) {
      // Remove added dims.
      input_dims[input_dims.size() - 1] = num_units;
      output_tensor = Reshape(params, output_tensor, input_dims);
    }
    params.outputs.push_back(output_tensor);
  }
};

// Uses MatMul (requires kEXPLICIT_BATCH)
// class DenseOpConverter : public TrtOpConverter {
// public:
//   DenseOpConverter() : TrtOpConverter({kTensor, kTensor}) { }

//   void Convert(AddTrtLayerParams& params) const {
//     const auto op0 = nvinfer1::MatrixOperation::kVECTOR;
//     const auto op1 = nvinfer1::MatrixOperation::kNONE;
//     nvinfer1::IMatrixMultiplyLayer* matmul_layer = params.network->addMatrixMultiply(*params.inputs.at(0).tensor, op0, *params.inputs.at(1).tensor, op1);
//     CHECK(matmul_layer != nullptr);
//     params.outputs.push_back(matmul_layer->getOutput(0));
//   }
// };

class BatchNormOpConverter : public TrtOpConverter {
public:
  BatchNormOpConverter() : TrtOpConverter({kTensor, kWeight, kWeight, kWeight, kWeight}) { }

  void Convert(AddTrtLayerParams& params) const {
    auto gamma = params.inputs.at(1).weight;
    auto beta = params.inputs.at(2).weight;
    auto mean = params.inputs.at(3).weight;
    auto var = params.inputs.at(4).weight;
    const auto* bn_attr = params.call->attrs.as<BatchNormAttrs>();
    CHECK_EQ(gamma.count, beta.count);
    CHECK_EQ(gamma.count, mean.count);
    CHECK_EQ(gamma.count, var.count);
    CHECK(bn_attr->axis == 1);

    // TODO(trevmorr): Properly manage memory.
    void* weight_scale_ptr = malloc(sizeof(float) * gamma.count);
    nvinfer1::Weights weight_scale{nvinfer1::DataType::kFLOAT, weight_scale_ptr, gamma.count};
    void* weight_shift_ptr = malloc(sizeof(float) * gamma.count);
    nvinfer1::Weights weight_shift{nvinfer1::DataType::kFLOAT, weight_shift_ptr, gamma.count};
    nvinfer1::Weights power{nvinfer1::DataType::kFLOAT, nullptr, 0};

    // fill in the content of weights for the Scale layer
    const float* gamma_ptr = reinterpret_cast<const float*>(gamma.values);
    const float* beta_ptr = reinterpret_cast<const float*>(beta.values);
    const float* mean_ptr = reinterpret_cast<const float*>(mean.values);
    const float* var_ptr = reinterpret_cast<const float*>(var.values);
    float* scale_ptr = reinterpret_cast<float*>(weight_scale_ptr);
    float* shift_ptr = reinterpret_cast<float*>(weight_shift_ptr);
    // TODO(trevmorr): consider parallelizing the following loop
    for (int i = 0; i < gamma.count; ++i) {
      scale_ptr[i] = 1.0 / std::sqrt(var_ptr[i] + bn_attr->epsilon);
      if (bn_attr->scale) {
        scale_ptr[i] *= gamma_ptr[i];
      }
      shift_ptr[i] = - mean_ptr[i] * scale_ptr[i];
      if (bn_attr->center) {
        shift_ptr[i] += beta_ptr[i];
      }
    }
    nvinfer1::IScaleLayer* scale_layer = params.network->addScale(
        *params.inputs.at(0).tensor, nvinfer1::ScaleMode::kCHANNEL, weight_shift, weight_scale, power);
    CHECK(scale_layer != nullptr);
    params.outputs.push_back(scale_layer->getOutput(0));
  }
};

class BatchFlattenOpConverter : public TrtOpConverter {
public:
  BatchFlattenOpConverter() : TrtOpConverter({kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    params.outputs.push_back(Reshape(params, params.inputs.at(0).tensor, {-1}));
  }
};

class SoftmaxOpConverter : public TrtOpConverter {
public:
  SoftmaxOpConverter() : TrtOpConverter({kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    const auto* softmax_attr = params.call->attrs.as<SoftmaxAttrs>();
    CHECK(softmax_attr->axis == -1);
    nvinfer1::ISoftMaxLayer* softmax_layer = params.network->addSoftMax(*params.inputs.at(0).tensor);
    CHECK(softmax_layer != nullptr);
    params.outputs.push_back(softmax_layer->getOutput(0));
  }
};

class PoolingOpConverter : public TrtOpConverter {
public:
  PoolingOpConverter() : TrtOpConverter({kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    auto input_tensor = params.inputs.at(0).tensor;
    auto input_dims = TrtDimsToVector(input_tensor->getDimensions());
    static const std::unordered_map<std::string, nvinfer1::PoolingType> op_map =
      {{"nn.max_pool2d", nvinfer1::PoolingType::kMAX},
       {"nn.avg_pool2d", nvinfer1::PoolingType::kAVERAGE}};
    auto it = op_map.find(params.op_name);
    CHECK(it != op_map.end()) << "Unsupported pooling type " << params.op_name << " in TensorRT";

    nvinfer1::DimsHW window_size, strides, padding;
    bool count_include_pad = false;
    if (params.op_name == "nn.max_pool2d") {
      const auto* pool_attr = params.call->attrs.as<MaxPool2DAttrs>();
      CHECK(pool_attr->layout == "NCHW");
      window_size = nvinfer1::DimsHW(pool_attr->pool_size[0].as<IntImm>()->value, pool_attr->pool_size[1].as<IntImm>()->value);
      strides = nvinfer1::DimsHW(pool_attr->strides[0].as<IntImm>()->value, pool_attr->strides[1].as<IntImm>()->value);
      padding = nvinfer1::DimsHW(pool_attr->padding[0].as<IntImm>()->value, pool_attr->padding[1].as<IntImm>()->value);
      CHECK(!pool_attr->ceil_mode);
    } else if (params.op_name == "nn.avg_pool2d") {
      const auto* pool_attr = params.call->attrs.as<AvgPool2DAttrs>();
      CHECK(pool_attr->layout == "NCHW");
      window_size = nvinfer1::DimsHW(pool_attr->pool_size[0].as<IntImm>()->value, pool_attr->pool_size[1].as<IntImm>()->value);
      strides = nvinfer1::DimsHW(pool_attr->strides[0].as<IntImm>()->value, pool_attr->strides[1].as<IntImm>()->value);
      padding = nvinfer1::DimsHW(pool_attr->padding[0].as<IntImm>()->value, pool_attr->padding[1].as<IntImm>()->value);
      CHECK(!pool_attr->ceil_mode);
      count_include_pad = pool_attr->count_include_pad;
    }

    auto pool_layer = params.network->addPooling(*input_tensor, it->second, window_size);
    CHECK(pool_layer != nullptr);
    pool_layer->setStride(strides);
    pool_layer->setPadding(padding);
    pool_layer->setAverageCountExcludesPadding(!count_include_pad);
    params.outputs.push_back(pool_layer->getOutput(0));
  }
};

class GlobalPoolingOpConverter : public TrtOpConverter {
public:
  GlobalPoolingOpConverter() : TrtOpConverter({kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    auto input_tensor = params.inputs.at(0).tensor;
    auto input_dims = TrtDimsToVector(input_tensor->getDimensions());
    static const std::unordered_map<std::string, nvinfer1::PoolingType> op_map =
      {{"nn.global_max_pool2d", nvinfer1::PoolingType::kMAX},
       {"nn.global_avg_pool2d", nvinfer1::PoolingType::kAVERAGE}};
    auto it = op_map.find(params.op_name);
    CHECK(it != op_map.end()) << "Unsupported pooling type " << params.op_name << " in TensorRT";
    const auto* pool_attr = params.call->attrs.as<GlobalPool2DAttrs>();
    CHECK(pool_attr->layout == "NCHW");
    const auto window_size = nvinfer1::DimsHW(input_dims[1], input_dims[2]);
    auto pool_layer = params.network->addPooling(*input_tensor, it->second, window_size);
    CHECK(pool_layer != nullptr);
    params.outputs.push_back(pool_layer->getOutput(0));
  }
};

class ExpandDimsOpConverter : public TrtOpConverter {
public:
  ExpandDimsOpConverter() : TrtOpConverter({kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    auto input_tensor = params.inputs.at(0).tensor;
    auto input_dims = TrtDimsToVector(input_tensor->getDimensions());
    const auto* expand_dims_attr = params.call->attrs.as<ExpandDimsAttrs>();
    CHECK(expand_dims_attr->axis > 0);
    // remove batch dim
    const int axis = expand_dims_attr->axis - 1;
    for (int i = 0; i < expand_dims_attr->num_newaxis; ++i) {
      input_dims.insert(input_dims.begin() + axis, 1);
    }
    params.outputs.push_back(Reshape(params, params.inputs.at(0).tensor, input_dims));
  }
};

class UnaryOpConverter : public TrtOpConverter {
public:
  UnaryOpConverter() : TrtOpConverter({kTensor}) { }

  void Convert(AddTrtLayerParams& params) const {
    // The following ops are supported by TRT but don't exist in relay yet:
    // recip, tan, sinh, cosh, asin, acos, asinh, acosh, atanh
    static const std::unordered_map<std::string, nvinfer1::UnaryOperation> op_map =
        {{"exp", nvinfer1::UnaryOperation::kEXP},
         {"log", nvinfer1::UnaryOperation::kLOG},
         {"sqrt", nvinfer1::UnaryOperation::kSQRT},
         {"abs", nvinfer1::UnaryOperation::kABS},
         {"negative", nvinfer1::UnaryOperation::kNEG},
         {"sin", nvinfer1::UnaryOperation::kSIN},
         {"cos", nvinfer1::UnaryOperation::kCOS},
         {"atan", nvinfer1::UnaryOperation::kATAN},
         {"ceil", nvinfer1::UnaryOperation::kCEIL},
         {"floor", nvinfer1::UnaryOperation::kFLOOR},
        };
    auto it = op_map.find(params.op_name);
    CHECK(it != op_map.end()) << "Unsupported unary type " << params.op_name;
    nvinfer1::IUnaryLayer* unary_layer =
        params.network->addUnary(*params.inputs.at(0).tensor, it->second);
    CHECK(unary_layer != nullptr);
    params.outputs.push_back(unary_layer->getOutput(0));
  }
};

// class BiasAddOpConverter : public TrtOpConverter {
// public:
//   BiasAddOpConverter() : TrtOpConverter({kTensor, kWeight}) { }

//   void Convert(AddTrtLayerParams& params) const {
//     nvinfer1::Weights shift{nvinfer1::DataType::kFLOAT, nullptr, 0};
//     nvinfer1::Weights power{nvinfer1::DataType::kFLOAT, nullptr, 0};
//     nvinfer1::IScaleLayer* scale_layer = params.network->addScale(
//       *params.inputs.at(0).tensor, nvinfer1::ScaleMode::kCHANNEL, params.inputs.at(1).weight, shift, power);
//     CHECK(scale_layer != nullptr);
//     params.outputs.push_back(scale_layer->getOutput(0));
//   }
// };

}  // namespace contrib
}  // namespace relay
}  // namespace tvm

#endif // TVM_RELAY_BACKEND_TRT_OPS_H_
